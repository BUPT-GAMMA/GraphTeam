## Data Preparation

In the `benchmark` folder, we have prepared benchmark files for different levels of tasks, namely `n=5`, `n=10`, and `n=20`. A new column `result` should be added to each benchmark file, which is used to store the answers generated by the large model.

## Pre-evaluation

After preparing the data, you need to fill in the correct paths in the `scripts/example/pre_evaluate.py` file and run it. This will generate the corresponding files and store them in the corresponding folders.

## Task Execution

After preparing the data and completing the pre-evaluation, you can directly run the `scripts/example/run_tasks.py` file. Please note that for different levels of tasks, you need to modify the `--log_dir` parameter in the `scripts/example/config` file to the corresponding folder.

Make sure you have prepared the data and completed the pre-evaluation before running the tasks.

## Closing Remarks

We hope this README can help you understand and use this project better. If you encounter any problems during use, please feel free to ask questions at any time.